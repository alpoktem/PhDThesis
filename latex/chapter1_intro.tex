%**************** Introduction **************
\chapter{Introduction}
\label{chapter:intro}
%Language, written vs spoken

%From spoken to written language. 
Written form of communication is much newer to the human than spoken communication. Even though it also constitutes a lesser fraction of the ways how humans communicate, it still is the basis of both traditional and computational linguistic studies. This is largely thanks to the discrete form that language took with the invention of the symbology to represent it. This symbology took many forms in time, earlier as images, icons and colors, later as abstract characters assembling words and sentences that constitute what we define as written language. This conversion of paradigm in language facilitated the recording, diffusion, study and automated processing of the human-made information. 

%Rich information in speech: prosody
Speech inherently carries much more information than the sounds and words that are uttered. Linguistic units like words are enveloped within properly divided measures, with a certain melody that has its highs and lows and even with a rhythm. These ``music-like'' aspects, which are defined as the \textit{prosody} in language, deals with \textit{how} a certain utterance is delivered. Through the prosodic structure of the language we are able to tell things both about the speaker and the information they want to transmit. For example, which sort of a mode they are in, if they are being serious, ironic or mocking, or where we should pay our attention in their discourse. 

%Encoding language to text
Even though an encoding of this dimension of speech seems missing in the written form of the language, it inherently accompanies them both on the way to being written and being read \citep{wallace}. In fact, as I am writing these words, I imagine explaining my thoughts to the reader with my inner voice prosody. In turn, I trust their own inner voices to enact what I am trying to say and form the same thoughts that I aim to transmit. Even in this form, although prosody is not as present and expressive as in human-to-human communication, words still resonate within ones own melody. 

%Encoding language to text means loss of information. However written language covers for it. 
However, one can notice the information loss during this information transformation. The reader can only guess how I want this discourse to sound like and that guess could be completely different than what I intend. A guess that is influenced by the general prosodic structure of the language (English in this case) the reader was exposed to with a bias on their own way of expressing it.  Written language gives only a limited set of symbolic tools to convey the information which would normally be encoded with prosody in spoken language. An example of this is punctuation, helping structuring intonational phrasing, marking modalities like question and exclamation, opening subordinate information flow and so on. This role of punctuation, i.e. substituting prosody, has evolved in time and established through grammatical rules and now is considered only partially affected by prosodic structure in language. In fact, written language in general evolved from spoken language so that a neutral prosody is sufficient in transferring of information through this limited symbology, thus minimizing the loss during the conversion. 

%Machines use symbols to communicate just like in written communication
Because of its discretized structure, written language has been much more convenient for machines to process since human has taught them ways to understand their language. This technological breakthrough, being defined as natural language processing (NLP), has opened ways to collaborate with machines to solve tasks such as translation, information extraction and transcription. These tasks can now be seen applied in great number of ways like spoken/written dialogue systems, automatic captioning, speech-to-speech translation, sentiment analysis etc. As machines function using a pre-defined set of symbols, language in its written form dominates most of these applications. Even in the cases where there is spoken language involved, machines rely on a step where speech has to be converted to text in order to carry on with the subsequent processes in the pipeline. 

Just like explained earlier, this conversion means a loss of modality in the language caused by the limited representation of it. An automatic speech transcription (ASR) system acting as the interface to these type of technologies merely transcribes "what is being said?" and discarding the information on "how is it being said?". This eventually affects the quality of the understanding the communicative intention in spoken language as a whole. If we desire to have machines understanding us in a complete way like humans do, this information residing in prosody needs to be considered besides the linguistic information. 

%Some info on prosody which I cannot develop \cite{Fujisaki1997}
%Prosody is realized through suprasegmental features of speech, which are intonation, tone, duration and rhythm and contributes, and contributes to the para-linguistic and non-lingustic content of the communication. It helps organize utterances into linguistic units using accentuation, phrasing and pausing. It helps define the modality of the sentence (question, assertion, command, 

\section{Motivation}

%Motivation and a short intro to prosody
The motivation of this dissertation is to explore ways to incorporate prosodic features of speech into systems that involve spoken language understanding. As sketched in the introductory section, the current tendency in these systems is carrying on with text processing once spoken input is converted into written form. This causes an irreversible loss of the information which is essential in human communication. This information is encoded through suprasegmental features of speech which are intonation, tone, duration and rhythm and contributes to the para-linguistic structure in communication. Examples of this para-linguistic structure are: phrasing that helps structure discourse, emphasis, modality defining if the communication is a question, assertion, exclamation or command, information structure that defines givenness etc. Systems that do not account for this information is bound to fail in reaching the level of a human in understanding natural language. 

%Application areas: (1) ASR and (2) SLMT
I particularly focus my research on two primal applications that involve conversion of speech into written form: Automatic speech transcription and spoken language machine translation. The former involves conversion of speech into written form and is applied in many type of applications such as dictation systems, automatic captioning and spoken dialogue systems. Spoken language machine translation (SLMT) involves automatic speech transcription as its first step but further translates the transcription into a second language and in some cases synthesize it. Its uses include, for example, automatic subtitling, automatic dubbing of broadcast media and speech-to-speech translation. This type of technology promises growth in the near future due to the huge influx of media created by internet users in the era of Web 2.0. While automated transcription facilitates indexing, tagging and accessibility for the hearing-impaired, automatic translation help reduce language barriers in accessing global information.

%TODO: Buraya neural networklerle ilgili bir paragraf gelsin.

Key to machine learning driven development is directly related to the availability of quality data. It shows that, development of speech data annotated with acoustic/prosodic features shows two main challenges. The first challenge is posed by the suprasegmental and multidimensional structure of prosody which makes alignment of linguistic and acoustic features difficult. Although some toolkits exist that assists prosodic annotation of speech data \citep{Rosenberg10}, they fall short in terms of applicability in machine learning based approaches. Second challenge is caused by the nature of prosody being directly influenced from context and environment where speech is taking place. Data collected in laboratory environments often fail to reflect expressivity normally present in spoken language, and in turn, influencing models to bias on unnatural data. To aid collection of expressive speech data, a form of harvesting ``found data'' that accommodates prosodic annotation is needed. Scalable methods to process, visualize and store this type of data is also necessary in the process of obtaining this type of data.

%More on Real-world use cases
%Three real-world use cases that use these technology are: automatic captioning, subtitling and automatic dubbing. Automatic captioning is the process of overlaying the transcription of the spoken content in the videos as it is or with cues to aid the hearing impaired. Subtitling is generally referred within a translation context where text is placed to aid watchers of a different language. This is employed through the channeling of ASR output to a machine translation system. One further step from this is automatic dubbing where a text-to-speech synthesizer in the translation language is employed and obtained speech is overlayed on the the original audio. These technologies help access of media content to the hearing impaired and diffusion of videos to other languages without the human labour of translation.

%ASR/punctuation motivation
The process of automatic speech transcription involves use of an automatic speech recognition (ASR) system to convert the spoken input to text. The raw text output of an ASR system generally lacks any form of punctuation. Depending on the application, punctuation proves to be important for two reasons: Firstly, in the cases where transcriptions will be read by humans, lack of punctuation reduces readability to a large extent. This is demonstrated in the work of \cite{Tundik2018} where watchers of broadcast news were asked to compare punctuated and unpunctuated captions. Both for manually and automatically created transcriptions, punctuated transcriptions were preferred in helping follow the video content. Second case where punctuation has an important role is when the ASR output is further used in subsequent processes like machine translation or parsing. Both these processes require sentence-like units as input and cannot function with long unsegmented text. Furthermore, commas and other punctuation marks that are defined within the orthography of the language pose as important cues in understanding and interpreting the text. Although rules of punctuation is formally defined within the grammatical and orthographic rules of the language, spoken language punctuation is predominantly influenced by prosody. Sentence structures and phrasing are often marked with intonational phrasing and breaks. Modality influences the intonation style of a sentence which in turn influences punctuation. Topic changes are marked with intensity and pitch resets. Emphasized information is delivered with a change in pitch or breaking. 

Looking at raw ASR output is often not enough in determining punctuation especially in cases of spontaneous speech. This type of speech often does not follow a regular syntactic structure as in written language, thus making it difficult to determine punctuation based on grammatical rules or data-driven methods that are modeled for written text \citep{ballesterosneural}. Neural network based work that gets use of prosodic cues report improvement in accuracy of the punctuation marks generated \citep{tilk2016bidirectional}, but still rely on huge chunks of textual data thus biasing models on written language. Also, there is no study reporting the effect of a wider variety of prosodic features into the task. 

%Translation motivation
Encoding of prosodic features in speech is most relevant within a speech-to-speech translation pipeline, which consists of the modules ASR, machine translation (MT) and text-to-speech TTS. This pipeline converts a spoken input in language A to text, translates its transcription to language B and then synthesizes it. Since both input and output are spoken language, conveying of prosodic features recognized in source input speech to the sythesized target has been a topic considered in these type of applications \citep{aguero2006prosody, Quoc2018}. In these approaches the limitations caused by the difficulty in harvesting expressive prosodic data is observed. Data collected in laboratory conditions do not serve sufficiently in covering a broad range of prosodic phenomena. For example these kind of limited approaches would not be sufficient in prosodic modelling within movie domain. 

\section{Objectives}
Enveloped in the motivation that prosody should be incorporated in technology that involves spoken language understanding, I have assigned the following objectives for the course of my research:

\begin{itemize}
    \item Development of tools that enable creation, prosodic annotation, handling and visualization of spoken language data, based on open sourced resources and publicly available,
    \item Development of an automated methodology based on open source tools for creating monolingual and bilingual prosodically annotated dataset from prosodically rich found data,
    \item Compilation and publication of monolingual and bilingual corpora suitable for machine learning based development that involves prosodic-linguistic modelling,
    \item Evaluation on the effect of various prosodic features on the task of automatic punctuation restoration and subsequent automatized processes like parsing and machine translation,
    \item Evaluation on the use of raw prosodic features on the task of spoken language machine translation. 
\end{itemize}

My hypothesis is that systems that take spoken language as input will benefit through the recovering of the lost prosodic information during the conversion of speech into text. The research and experimentation involved in this dissertation have this aim in general. 

%Experiments based on neural nets 
\section{Outlining the Dissertation}

The dissertation is structured in six chapters as follows:

\begin{itemize}
    \item \textbf{Chapter 2} first gives an overview on speech processing systems encompassing the motivated applications of this dissertation. Emphasis is given on deep neural network based setups. Second, state-of-the-art work is presented in the specific topics which this dissertation focuses: Punctuation restoration in automatically generated transcripts and utilization of prosody in spoken language machine translation. 
    \item \textbf{Chapter 3} presents methodologies revolving around prosodically annotated corpus compilation and visualization and the corpora used and published within the dissertation's frame.
    \item \textbf{Chapter 4} focuses on the theme of punctuation restoration in raw speech transcripts with a focus on the use of prosodic features. Experiments involve testing of various feature sets incorporated into a punctuation restoration pipeline, evaluation for dependency parsing and accompanying an ASR system.
    \item \textbf{Chapter 5} explores the use of prosodic features within a neural spoken machine translation setting. An expressive parallel corpus collected from a TV series is used to do prosodic adaptation on movie-domain translation. Incorporation of prosody into both the encoding and decoding modules of a speech-to-speech translation pipeline is experimented.
    \item \textbf{Chapter 6} puts the final conclusions on the thesis in terms of the objectives reached and also outlines possible venues for future research and applications. 
\end{itemize}


