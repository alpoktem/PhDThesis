%**************** Introduction ******************
\chapter{Introduction}
\label{chapter:intro}

%Background: technology that converts speech into written form
%background1: human machine colaboration
Human machine collaboration has arrived to another level with the advent of natural language processing (NLP). By teaching machines to understand and interpret human languages, we can now solve many problems that before required human labour. For example, dialog systems help solve our queries the same way we are used to interact with a person, or machine translation has changed the way we perceive language barriers. Technology like automatic speech recognition and text-to-speech synthesis has made this interaction further possible in the spoken form of human languages. Automatic speech transcription, for example, involves conversion of speech to its written form and is applied in many type of applications such as dictation systems, automatic captioning and spoken dialogue systems. Spoken language machine translation (SLMT) involves automatic speech transcription as its first step but further translates the transcription into a second language and in some cases synthesize it. Its uses include, for example, automatic subtitling and automatic dubbing.

%Problem: During conversion, information is lost
As machines function using a pre-defined set of symbols, language in its written form dominates the functioning of most of these applications. Even in the cases where spoken language is involved, machines rely on a step where speech has to be converted to text in order to carry on with the subsequent processes in the pipeline. However, this conversion brings with it a loss of a dimension in the language. Compared to written language, spoken language inherently carries more information than its linguistic content. Linguistic units like words are enveloped within properly divided measures, accompanied with a certain melody that has its ups and downs delivered in a rhythm. These ``music-like'' aspects, which roughly correspond to ``prosody'' in language, deal with \textit{how} a certain utterance is delivered. It functions for structuring the spoken discourse and also to encode both linguistic and para-linguistic phenomena \citep{fujisaki2004}. Loss of this information in spoken language interfaced systems eventually harms the machine interpretation of the communicative intention as a whole. 

%This dissertation 
%If we desire to have machines understanding us in a complete way like humans do, this information residing in prosody needs to be processed besides the linguistic information.

\section{Motivation}

%Motivation and a short intro to prosody

The recent advances in automated processing of natural languages owe much to the application of neural networks. Although the core technology behind neural networks is not new, it has only recently started being preferred against older probabilistic methods in NLP and speech technology. This was largely due to the rise in computational power that made possible the training and applicability of the systems that use \textit{deep neural networks (DNN)}. Its use for language modelling was demonstrated in 2001 \citep{Bengio:LM}, for automatic speech recognition (ASR) in 2012 \citep{asr_dnnhmm} and machine translation (MT) in 2014 \citep{sutskever}. %Popularly used machine translation service \textit{Google Translate}\footnote{\url{http://translate.google.com}} uses DNN at its core since 2016 \citep{google_nmt}. 

As sketched in the introductory section, the current tendency in spoken language processing systems is carrying on with the modelling only the linguistic information once spoken input is converted to its written form. This causes an irreversible loss of the information that is encoded through prosodic features of speech, which are intonation, rhythm, and stress. 

I will demonstrate in this section the relevance of modelling of this level of language in two applications of spoken language processing: Automatic Speech Transcription and Spoken Language Machine Translation. Finally, I will touch on the issue of prosodic data compilation which is demanded in development of data-driven models that account for prosody. 

\subsection{Automatic Speech Transcription}
%ASR/punctuation motivation
The process of automatic speech transcription involves use of an ASR system to convert the spoken input to text. The raw text output of an ASR system generally lacks any form of punctuation. Depending on the application, punctuation proves to be important for two reasons: first, in the cases where transcriptions will be read by humans, lack of punctuation reduces readability to a large extent. This is demonstrated in the work of \cite{Tundik2018} where watchers of broadcast news were asked to compare punctuated and unpunctuated captions. Both for manually and automatically created transcriptions, punctuated transcriptions were preferred in helping follow the video content. Second case where punctuation has an important role is when the ASR output is further used in subsequent processes like machine translation or parsing. Both these processes require sentence-like units as input and cannot function with long unsegmented text. Furthermore, commas and other punctuation marks that are defined within the orthography of a language prove to be important cues for machines to understand text, similar to the case of humans \citep{Cho2017NMTbasedSA, Jones:1994:ERP:991886.991960}. Although rules of punctuation are formally defined within the grammatical and orthographic rules of a language, spoken language punctuation is predominantly related with prosody \citep{wallace}. Sentence structures and phrasing are often marked with intonational phrasing and breaks. Sentence modality influences the intonation style of a sentence, which in turn influences punctuation. Topic changes are generally marked with intensity and pitch resets \citep{Farrus:SP:2016}. Emphasized information is often delivered with stress. 

Looking at raw ASR output, which consists of only the linguistic content, is often not enough to determine punctuation especially in cases of spontaneous speech. This type of speech often does not follow a regular syntactic structure as in written language, thus making it difficult to determine punctuation based on syntactic or data-driven methods that are modeled for written text \citep{ballesterosneural}. Neural network-based work that gets use of prosodic cues report improvement in accuracy of the punctuation marks generated \citep{tilk2016bidirectional}, but still rely on huge chunks of textual data thus biasing models on written language. As they are the closest form of symbology that represents speech prosody in written form of language, its modelling requires a level in prosody. This calls for further study in the evaluation of various prosodic features on the task.  

\subsection{Machine Translation Enhancement with Prosody}
%Translation motivation
Spoken language machine translation (SLMT) is a type of machine translation architecture where input and/or output to the system is spoken language. In the speech-input setup, prosody is relevant for capturing the sentence structure and phrasing which in turn affects translations. In spoken-output systems the need to convey the prosodic structure into the synthesized speech appears in applications such as automatic dubbing. In both setups, a prosodic modelling of the input speech is needed to avoid the information loss at the recognition step. Prosodic transfer modelling was previously explored in a number of works \citep{aguero2006prosody, Quoc2018, anumanchipalli:2012}. The data used in these approaches are collected in laboratory conditions, meaning that recordings are prompted, and almost always are based on travel domain. There is no previous study that takes on a domain that involves more expressive speech such as movies or TV shows. Especially in these domains, there is a rich source of prosodic varieties that affect both translations for subtitling and dubbing. 

\subsection{Prosodic Data Compilation}
%Prosodic data collection
Key to machine learning-driven development is directly related to the availability of quality data. Although some toolkits exist that assist prosodic annotation of speech data \citep{Rosenberg10, ProsodyPro, L06-1340}, they fall short in terms of applicability in machine learning-based approaches. Another issue in prosodic data collection is that data collected in laboratory environments often fail in reflecting expressivity normally present in spoken language, and in turn, influence models to bias on unnatural data. To aid collection of expressive speech data, a form of harvesting ``found data'' that accommodates prosodic annotation is needed. Scalable methods to process, visualize and store this type of data is also necessary in developing data-driven methodologies.

\section{Objectives}
Revolving around the motivation that prosody should be incorporated in technology that involves spoken language understanding and processing, I have assigned the following objectives for the course of my research: 

\begin{itemize}
    \item Development of open tools that enable creation, prosodic annotation, handling and visualization of spoken language data.
    \item Compilation and publication of monolingual and bilingual corpora suitable for machine learning-based development that involves prosodic-linguistic modelling.
    \item Development of a framework for automatic punctuation restoration in  manually or automatically generated speech transcripts, using lexical and prosodic features.
    \item Assessment of the effect of acoustic-prosodic features on the quality of punctuation restoration and subsequent processes like dependency parsing and machine translation.
    \item Development of a machine translation framework for movie domain that enables prosodic feature input and output to aid translation and also to generate cues for synthesis.
\end{itemize}

My hypothesis is that systems that process spoken language will benefit from  modelling of prosodic features in speech besides the linguistic modelling involved in them. The experiments aim to assess this in neural network-based architectures. 

%Experiments based on neural nets 
\section{Outlining the Dissertation}

The rest of this dissertation is structured as follows:

\begin{itemize}
    \item \textbf{Chapter 2} first gives an overview on speech processing systems encompassing the motivated applications of this dissertation. Emphasis is given on deep neural network based-setups. Second, role and characteristics of speech prosody are reviewed. Finally, state-of-the-art in relation to my objectives are presented. 
    \item \textbf{Chapter 3} presents corpus related work. Tools that were developed and utilized for prosodic data compilation and visualization are presented. Also, two corpora that were used and published within the frame of this dissertation are explained in detail. 
    \item \textbf{Chapter 4} focuses on the topic of automatic punctuation restoration in raw speech transcripts with a focus on the use of prosodic features and their effects on recovery performance and parsing.
    \item \textbf{Chapter 5} explores the use of prosodic features within a neural spoken machine translation setting. 
    \item \textbf{Chapter 6} sets the final conclusions on the thesis in terms of the objectives reached and also outlines possible venues for future research and applications. 
\end{itemize}


